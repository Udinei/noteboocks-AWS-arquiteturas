{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fded102b",
   "metadata": {},
   "source": [
    "# Tarefa 2a: resumo de textos com arquivos pequenos com o Titan Text Premier\n",
    "\n",
    "\n",
    "Neste caderno, voc√™ vai ingerir uma pequena sequ√™ncia de texto diretamente na API do Amazon Bedrock (usando o modelo do Titan Text) e vai instru√≠-la a resumir o texto de entrada. Voc√™ pode aplicar essa abordagem para resumir transcri√ß√µes de chamadas, transcri√ß√µes de reuni√µes, livros, artigos, publica√ß√µes de blogs e outros conte√∫dos relevantes quando o tamanho do texto de entrada estiver dentro dos limites de tamanho do contexto do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca28279c",
   "metadata": {},
   "source": [
    "## Tarefa 2a.1: configurar o ambiente\n",
    "\n",
    "Nesta tarefa, voc√™ configurar√° o ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66edf151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create a service client by name using the default session.\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "\n",
    "bedrock_client = boto3.client('bedrock-runtime',region_name=os.environ.get(\"AWS_DEFAULT_REGION\", None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da4d9ee",
   "metadata": {},
   "source": [
    "\n",
    "## Tarefa 2a.2: escrever prompt com o texto a ser resumido\n",
    "\n",
    "Nesta tarefa, voc√™ usar√° um pequeno trecho do texto com menos tokens do que o tamanho m√°ximo permitido pelo modelo de base. Como exemplo de texto de entrada para este laborat√≥rio, voc√™ usa um par√°grafo de uma [postagem do Blog da AWS] (https://aws.amazon.com/jp/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/) que anuncia o Amazon Bedrock.\n",
    "\n",
    "O prompt come√ßa com uma instru√ß√£o `Please provide a summary of the following text.`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ece0c069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"\n",
    "\n",
    "Por favor, forne√ßa um resumo do seguinte texto:\n",
    "\n",
    "A AWS analisou todos os feedbacks dos clientes e hoje temos o prazer de anunciar o Amazon Bedrock, \\\n",
    "um novo servi√ßo que torna os FMs da AI21 Labs, Anthropic, Stability AI e Amazon acess√≠veis por meio de uma API. \\\n",
    "O Bedrock √© a maneira mais f√°cil para os clientes criarem e escalarem aplicativos baseados em IA generativa usando FMs, \\\n",
    "democratizando o acesso para todos os desenvolvedores. \\\n",
    "O Bedrock oferecer√° a capacidade de acessar uma variedade de FMs poderosos para texto e imagens \\\n",
    " ‚Äî incluindo os FMs Titan da Amazon, que consistem em dois novos LLMs que tamb√©m estamos anunciando hoje \\\n",
    " ‚Äî por meio de um servi√ßo gerenciado da AWS escal√°vel, confi√°vel e seguro. Com a experi√™ncia serverless da Bedrock, \\\n",
    "os clientes podem encontrar facilmente o modelo certo para o que est√£o tentando realizar, come√ßar rapidamente, \\\n",
    "personalizar os FMs de forma privada com seus pr√≥prios dados e integr√°-los e implant√°-los facilmente em seus \\\n",
    "aplicativos usando as ferramentas e recursos da AWS com os quais j√° est√£o familiarizados, sem precisar gerenciar \\\n",
    "nenhuma infraestrutura (incluindo integra√ß√µes com recursos do Amazon SageMaker ML, como Experimentos para testar \\\n",
    "diferentes modelos e Pipelines para gerenciar seus FMs em escala).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efddbb0",
   "metadata": {},
   "source": [
    "## Tarefa 2a.3: Criar corpo da solicita√ß√£o com par√¢metros de infer√™ncia e prompt \n",
    "\n",
    "Nesta tarefa, voc√™ criar√° o corpo do prompt com os par√¢metros de infer√™ncia e prompt acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "60d191eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# request body - Amazon Nova format\n",
    "body = json.dumps({\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": prompt_data}]\n",
    "        }\n",
    "    ],\n",
    "    \"inferenceConfig\": {\n",
    "        \"maxTokens\": 1000,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9f3326",
   "metadata": {},
   "source": [
    "## Tarefa 2a.4: Invocar o modelo de base via Boto3\n",
    "\n",
    "Nesta tarefa, voc√™ enviar√° uma solicita√ß√£o de API para o Amazon Bedrock especificando os par√¢metros da solicita√ß√£o: `modelId`, `accept` e `contentType`. Ap√≥s o prompt fornecido, o modelo de base no Amazon Bedrock resume o texto de entrada.\n",
    "\n",
    "### Concluir gera√ß√£o de sa√≠da\n",
    "\n",
    "Por padr√£o, o servi√ßo Amazon Bedrock gera o resumo completo de um determinado prompt em uma √∫nica sa√≠da. Pode demorar se a sa√≠da do modelo contiver muitos tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9f400d76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estrutura da resposta: dict_keys(['output', 'stopReason', 'usage'])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "A AWS anunciou o lan√ßamento do Amazon Bedrock, um novo servi√ßo que facilita o acesso a modelos de intelig√™ncia artificial (IA) de empresas como AI21 Labs, Anthropic, Stability AI e da pr√≥pria Amazon por meio de uma API. O Bedrock visa democratizar o uso de IA generativa, permitindo que desenvolvedores criem e escalem aplicativos com modelos de linguagem (LLMs) e modelos de imagem de forma r√°pida e segura. O servi√ßo oferece modelos gerenciados pela AWS, incluindo os novos modelos Titan da Amazon, e elimina a necessidade de gerenciar infraestrutura. Com integra√ß√µes com o Amazon SageMaker, os usu√°rios podem testar e gerenciar modelos em escala."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#model configuration and invoke the model\n",
    "modelId = 'amazon.nova-lite-v1:0' # Amazon Nova \n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n",
    "outputText = \"\\n\"\n",
    "\n",
    "try:\n",
    "    response = bedrock_client.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    \n",
    "    # Nova usa formato diferente - verifique a estrutura da resposta\n",
    "    print(\"Estrutura da resposta:\", response_body.keys())\n",
    "    \n",
    "    # Tente extrair o texto (formato pode variar)\n",
    "    if 'output' in response_body:\n",
    "        outputText = response_body['output']['message']['content'][0]['text']\n",
    "    elif 'outputText' in response_body:\n",
    "        outputText = response_body['outputText']\n",
    "    else:\n",
    "        print(\"Resposta completa:\", response_body)\n",
    "        \n",
    "    #print(outputText)\n",
    "    display(Markdown(outputText))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a5592e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìä RESUMO DE TOKENS\n",
      "==================================================\n",
      "Tokens de entrada:  289\n",
      "Tokens de sa√≠da:    184\n",
      "Total:              473\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exibir contagem de tokens\n",
    "if 'usage' in response_body:\n",
    "    input_tokens = response_body['usage'].get('inputTokens', 0)\n",
    "    output_tokens = response_body['usage'].get('outputTokens', 0)\n",
    "    total_tokens = input_tokens + output_tokens\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"üìä RESUMO DE TOKENS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Tokens de entrada:  {input_tokens}\")\n",
    "    print(f\"Tokens de sa√≠da:    {output_tokens}\")\n",
    "    print(f\"Total:              {total_tokens}\")\n",
    "    print(f\"{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180c84a0",
   "metadata": {},
   "source": [
    "### Transmitir gera√ß√£o de sa√≠da\n",
    "\n",
    "Em seguida, voc√™ ver√° como usar a API invoke_model_with_response_stream do Amazon Bedrock para transmitir sa√≠das do modelo para que os usu√°rios possam consumi-las conforme s√£o geradas. Em vez de gerar a sa√≠da completa de uma s√≥ vez, essa API retorna um ResponseStream que envia blocos de sa√≠da menores do modelo conforme s√£o produzidos. Voc√™ pode exibir essas sa√≠das de transmiss√£o em uma visualiza√ß√£o cont√≠nua e consum√≠vel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea3aa446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'chunk': {'bytes': b'{\"messageStart\":{\"role\":\"assistant\"}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"A\"},\"contentBlockIndex\":0}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":0}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" AWS anuncio\"},\"contentBlockIndex\":1}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":1}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"u o\"},\"contentBlockIndex\":2}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":2}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" Amazon Bedrock, um\"},\"contentBlockIndex\":3}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":3}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" servi\\xc3\\xa7o\"},\"contentBlockIndex\":4}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":4}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" que\"},\"contentBlockIndex\":5}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":5}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" permite aos\"},\"contentBlockIndex\":6}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":6}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" clientes aces\"},\"contentBlockIndex\":7}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":7}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"sar e\"},\"contentBlockIndex\":8}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":8}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" utilizar\"},\"contentBlockIndex\":9}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":9}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" modelos\"},\"contentBlockIndex\":10}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":10}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" de IA\"},\"contentBlockIndex\":11}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":11}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" (\"},\"contentBlockIndex\":12}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":12}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"FM\"},\"contentBlockIndex\":13}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":13}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"s)\"},\"contentBlockIndex\":14}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":14}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" de diversas\"},\"contentBlockIndex\":15}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":15}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" fonte\"},\"contentBlockIndex\":16}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":16}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"s,\"},\"contentBlockIndex\":17}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":17}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" incluindo AI\"},\"contentBlockIndex\":18}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":18}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"21 Labs\"},\"contentBlockIndex\":19}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":19}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\", Anthrop\"},\"contentBlockIndex\":20}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":20}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"ic, Stability\"},\"contentBlockIndex\":21}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":21}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" AI e\"},\"contentBlockIndex\":22}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":22}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" Amazon, por\"},\"contentBlockIndex\":23}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":23}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" meio de uma API\"},\"contentBlockIndex\":24}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":24}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\". O\"},\"contentBlockIndex\":25}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":25}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" Bedrock facilita\"},\"contentBlockIndex\":26}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":26}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" a cria\\xc3\\xa7\\xc3\\xa3o\"},\"contentBlockIndex\":27}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":27}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" e\"},\"contentBlockIndex\":28}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":28}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" a\"},\"contentBlockIndex\":29}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":29}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" escala de\"},\"contentBlockIndex\":30}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":30}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" aplicativo\"},\"contentBlockIndex\":31}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":31}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"s base\"},\"contentBlockIndex\":32}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":32}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"ados em IA genera\"},\"contentBlockIndex\":33}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":33}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"tiva, democrat\"},\"contentBlockIndex\":34}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":34}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"izando o\"},\"contentBlockIndex\":35}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":35}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" acesso a modelos\"},\"contentBlockIndex\":36}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":36}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" de texto\"},\"contentBlockIndex\":37}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":37}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" e imagem\"},\"contentBlockIndex\":38}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":38}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" poderoso\"},\"contentBlockIndex\":39}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":39}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"s. O servi\\xc3\\xa7o\"},\"contentBlockIndex\":40}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":40}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" oferece\"},\"contentBlockIndex\":41}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":41}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" uma\"},\"contentBlockIndex\":42}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":42}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" experi\\xc3\\xaancia\"},\"contentBlockIndex\":43}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":43}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" server\"},\"contentBlockIndex\":44}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":44}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"less, permit\"},\"contentBlockIndex\":45}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":45}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"indo que\"},\"contentBlockIndex\":46}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":46}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" os desenvolve\"},\"contentBlockIndex\":47}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":47}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"dores encontr\"},\"contentBlockIndex\":48}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":48}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"em os\"},\"contentBlockIndex\":49}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":49}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" modelos certo\"},\"contentBlockIndex\":50}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":50}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"s, personalize\"},\"contentBlockIndex\":51}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":51}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"m-\"},\"contentBlockIndex\":52}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":52}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"nos\"},\"contentBlockIndex\":53}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":53}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" com seus\"},\"contentBlockIndex\":54}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":54}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" dados\"},\"contentBlockIndex\":55}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":55}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" e \"},\"contentBlockIndex\":56}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":56}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"integrem\"},\"contentBlockIndex\":57}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":57}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"-nos\"},\"contentBlockIndex\":58}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":58}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" facilmente\"},\"contentBlockIndex\":59}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":59}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" em seus aplicativos,\"},\"contentBlockIndex\":60}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":60}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" utilizando\"},\"contentBlockIndex\":61}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":61}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" as ferramentas da\"},\"contentBlockIndex\":62}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":62}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" AWS.\"},\"contentBlockIndex\":63}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":63}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" O Bed\"},\"contentBlockIndex\":64}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":64}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"rock inclui\"},\"contentBlockIndex\":65}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":65}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" os\"},\"contentBlockIndex\":66}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":66}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" novos\"},\"contentBlockIndex\":67}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":67}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" modelos Titan\"},\"contentBlockIndex\":68}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":68}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" da Amazon e\"},\"contentBlockIndex\":69}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":69}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" integra\"},\"contentBlockIndex\":70}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":70}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"-se com recursos do\"},\"contentBlockIndex\":71}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":71}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" Amazon\"},\"contentBlockIndex\":72}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":72}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" SageMaker para\"},\"contentBlockIndex\":73}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":73}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" test\"},\"contentBlockIndex\":74}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":74}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"es\"},\"contentBlockIndex\":75}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":75}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" e gerencia\"},\"contentBlockIndex\":76}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":76}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\"mento de modelos em\"},\"contentBlockIndex\":77}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":77}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockDelta\":{\"delta\":{\"text\":\" escala.\"},\"contentBlockIndex\":78}}'}},\n",
       " {'chunk': {'bytes': b'{\"contentBlockStop\":{\"contentBlockIndex\":78}}'}},\n",
       " {'chunk': {'bytes': b'{\"messageStop\":{\"stopReason\":\"end_turn\"}}'}},\n",
       " {'chunk': {'bytes': b'{\"metadata\":{\"usage\":{\"inputTokens\":289,\"outputTokens\":163,\"cacheReadInputTokenCount\":0,\"cacheWriteInputTokenCount\":0},\"metrics\":{},\"trace\":{}},\"amazon-bedrock-invocationMetrics\":{\"inputTokenCount\":289,\"outputTokenCount\":163,\"invocationLatency\":1126,\"firstByteLatency\":101,\"cacheReadInputTokenCount\":0,\"cacheWriteInputTokenCount\":0}}'}}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#invoke model with response stream\n",
    "modelId = 'amazon.nova-lite-v1:0'\n",
    "response = bedrock_client.invoke_model_with_response_stream(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "\n",
    "stream = response.get('body')\n",
    "output = list(stream)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01ab3461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_markdown,Markdown,clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f0148858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando stream da resposta...\n",
      "\n",
      "Total tokens: 458\n",
      "\n",
      "\t\t\u001b[31m**RESUMO COMPLETO**\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "A AWS anunciou o lan√ßamento do Amazon Bedrock, um servi√ßo que facilita o acesso e uso de modelos de intelig√™ncia artificial (IA) de diversas empresas, incluindo AI21 Labs, Anthropic, Stability AI e Amazon, atrav√©s de uma API. Este servi√ßo tem como objetivo democratizar o uso de IA generativa para desenvolvedores de todas as habilidades, permitindo que eles criem e escalem aplicativos baseados em IA de forma mais f√°cil. O Bedrock oferece acesso a uma variedade de modelos de linguagem (LLMs) e de gera√ß√£o de imagens, incluindo os novos modelos Titan da Amazon, tudo gerenciado por um servi√ßo escal√°vel, confi√°vel e seguro da AWS. Os usu√°rios podem facilmente encontrar modelos adequados para suas necessidades, come√ßar rapidamente, personalizar os modelos com seus pr√≥prios dados e integrar esses modelos em seus aplicativos usando as ferramentas e recursos familiares da AWS, sem a necessidade de gerenciar a infraestrutura subjacente. O servi√ßo tamb√©m inclui integra√ß√µes com recursos do Amazon SageMaker para testes e gerenciamento de modelos em escala."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelId = 'amazon.nova-lite-v1:0'\n",
    "response = bedrock_client.invoke_model_with_response_stream(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "stream = response.get('body')\n",
    "output = []\n",
    "i = 1\n",
    "\n",
    "print(\"Processando stream da resposta...\\n\")\n",
    "if stream:\n",
    "    for event in stream:\n",
    "        chunk = event.get('chunk')\n",
    "        if chunk:\n",
    "            chunk_obj = json.loads(chunk.get('bytes').decode())\n",
    "            \n",
    "            # Processa contentBlockDelta (texto real)\n",
    "            if 'contentBlockDelta' in chunk_obj:\n",
    "                text = chunk_obj['contentBlockDelta']['delta']['text']\n",
    "                output.append(text)\n",
    "                i += 1\n",
    "            \n",
    "            # Processa metadata (tokens)\n",
    "            elif 'metadata' in chunk_obj:\n",
    "                print(f\"Total tokens: {usage.get('inputTokens', 0) + usage.get('outputTokens', 0)}\")\n",
    "\n",
    "# Exibir sa√≠da completa formatada\n",
    "print('\\n\\t\\t\\x1b[31m**RESUMO COMPLETO**\\x1b[0m\\n')\n",
    "complete_output = ''.join(output)\n",
    "display(Markdown(complete_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e8ee83",
   "metadata": {},
   "source": [
    "Voc√™ testou o uso do SDK boto3 para acessar a API do Amazon Bedrock. Esse SDK fornece acesso program√°tico b√°sico aos recursos do Bedrock. Ao usar essa API, voc√™ conseguiu implementar dois casos de uso: 1) Gerar um resumo de texto completo do conte√∫do de not√≠cias da AWS de uma s√≥ vez e 2) Transmitir a sa√≠da resumida em blocos para processamento incremental.\n",
    "\n",
    "### Experimente voc√™ mesmo\n",
    "- Altere os prompts para seu caso de uso espec√≠fico e avalie o resultado de diferentes modelos.\n",
    "- Teste o comprimento do token para entender a lat√™ncia e a responsividade do servi√ßo.\n",
    "- Aplique diferentes princ√≠pios de engenharia de prompts para gerar resultados melhores.\n",
    "\n",
    "### Limpeza\n",
    "\n",
    "Voc√™ concluiu este caderno. Passe para a pr√≥xima parte do laborat√≥rio da seguinte forma:\n",
    "\n",
    "- Feche este arquivo de caderno e continue com **Task2b.ipynb**."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
