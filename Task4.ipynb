{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarefa 4: interface conversacional: conversar com os LLMs Llama 3 e Titan Premier\n",
    "\n",
    "Neste caderno, você criará um chatbot usando os modelos de base (FMs) llama3-8b-instruct e titan-text-premier no Amazon Bedrock.\n",
    "\n",
    "Interfaces conversacionais, como chatbots e assistentes virtuais, podem aprimorar a experiência do usuário para seus clientes. Os chatbots usam algoritmos de processamento de linguagem natural (PLN) e machine learning para entender e responder às consultas dos usuários. Você pode usar chatbots em diversas aplicações, como atendimento ao cliente, vendas e comércio eletrônico, para oferecer respostas rápidas e eficientes aos usuários. Os usuários podem acessá-los por vários canais, como sites, plataformas de redes sociais e aplicativos de mensagens.\n",
    "\n",
    "- **Chatbot (básico)**, chatbot zero-shot com modelo FM\n",
    "- **Chatbot com uso de prompt**, template(LangChain): chatbot com algum contexto fornecido no modelo de prompt\n",
    "- **Chatbot com persona**: chatbot com funções definidas, por exemplo, coach de carreira e interações humanas\n",
    "- **Chatbot com consciência contextual**, inclusão de contexto por meio de um arquivo externo gerando incorporações.\n",
    "\n",
    "## Framework LangChain para criação de chatbot com o Amazon Bedrock\n",
    "\n",
    "Em interfaces conversacionais, como chatbots, lembrar as interações anteriores é muito importante, tanto no curto quanto no longo prazo.\n",
    "\n",
    "O framework LangChain fornece componentes de memória em duas formas. Primeiro, o LangChain oferece recursos auxiliares para gerenciar e manipular mensagens de chat anteriores. Eles são projetados para serem modulares. Em segundo lugar, o LangChain fornece maneiras fáceis de incorporar esses utilitários em cadeias, permitindo que você defina e interaja facilmente com diferentes tipos de abstrações, o que facilita a criação de chatbots avançados.\n",
    "\n",
    "## Criar chatbot com contexto: elementos principais\n",
    "\n",
    "O primeiro processo na criação de um chatbot com consciência contextual é gerar incorporações para o contexto. Normalmente, ocorre um processo de ingestão no seu modelo para gerar as incorporações, que depois são inseridas no armazenamento de vetores. Neste caderno, você usa o modelo do Titan Embeddings para isso. O segundo processo é a orquestração, interação, invocação e o retorno dos resultados das solicitações dos usuários. Isso envolve orquestrar a solicitação do usuário, interagir com os modelos/componentes necessários para coletar informações, invocar o chatbot para formular uma resposta e, em seguida, retornar a resposta do chatbot ao usuário."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarefa 4.1: Configurar o ambiente\n",
    "\n",
    "Nesta tarefa, você configurará o ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ignore warnings and create a service client by name using the default session.\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import boto3\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "bedrock_client = boto3.client('bedrock-runtime',region_name=os.environ.get(\"AWS_DEFAULT_REGION\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format instructions into a conversational prompt\n",
    "from typing import Dict, List\n",
    "\n",
    "def format_instructions(instructions: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"Formate as instruções onde os papéis na conversa devem se alternar system/user/assistant/user/assistant/...\"\"\"\n",
    "    prompt: List[str] = []\n",
    "    for instruction in instructions:\n",
    "        if instruction[\"role\"] == \"system\":\n",
    "            prompt.extend([\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\", (instruction[\"content\"]).strip(), \" <|eot_id|>\"])\n",
    "        elif instruction[\"role\"] == \"user\":\n",
    "            prompt.extend([\"<|start_header_id|>user<|end_header_id|>\\n\", (instruction[\"content\"]).strip(), \" <|eot_id|>\"])\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid role: {instruction['role']}. Role must be either 'user' or 'system'.\")\n",
    "    prompt.extend([\"<|start_header_id|>assistant<|end_header_id|>\\n\"])\n",
    "    return \"\".join(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tarefa 4.2: usar o histórico do chat do LangChain para iniciar a conversa\n",
    "\n",
    "Nesta tarefa, você ativará o chatbot para que guarde o contexto das conversas entre as várias interações com os usuários. Ter uma memória conversacional é imprescindível para que os chatbots mantenham diálogos significativos e coerentes ao longo do tempo.\n",
    "\n",
    "Você implementa recursos de memória conversacional com base na classe InMemoryChatMessageHistory do LangChain. Esse objeto armazena as conversas entre o usuário e o chatbot, e o histórico fica disponível para que o agente do chatbot possa aproveitar o contexto de uma conversa anterior.\n",
    "\n",
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Nota:** as saídas do modelo não são determinísticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**HUMAN:** como vai você?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** Eu não tenho sentimentos ou emoções, mas agradeço por perguntar! Estou aqui e pronto para ajudar com qualquer dúvida ou informação que você precise. Como posso ser útil para você hoje? Se houver algo específico que você queira saber ou discutir, é só me avisar!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_aws import ChatBedrock\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "chat_model=ChatBedrock(\n",
    "    model_id=\"us.amazon.nova-premier-v1:0\" , \n",
    "    client=bedrock_client)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Responda às seguintes perguntas da melhor forma possível.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = InMemoryChatMessageHistory()\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return history\n",
    "\n",
    "\n",
    "chain = prompt | chat_model | StrOutputParser()\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_history,\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "query=\"como vai você?\"\n",
    "response=wrapped_chain.invoke({\"input\": query})\n",
    "# Printing history to see the history being built out. \n",
    "for msg in history.messages:\n",
    "    display(Markdown(f\"**{msg.type.upper()}:** {msg.content}\"))\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# For the rest of the conversation, the output will only include response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novas perguntas\n",
    "\n",
    "O modelo respondeu com uma mensagem inicial. Agora, você faz algumas perguntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Claro! Começar uma horta pode ser uma experiência gratificante. Aqui estão algumas dicas para ajudar você a iniciar:\n",
       "\n",
       "1. **Escolha o Local**:\n",
       "   - **Sol**: A maioria das plantas precisa de pelo menos 6 horas de luz solar direta por dia.\n",
       "   - **Acesso à Água**: Proximidade a uma fonte de água facilita a irrigação.\n",
       "\n",
       "2. **Planejamento**:\n",
       "   - **Tipos de Plantas**: Decida quais vegetais, ervas ou frutas você quer cultivar. Considere o clima da sua região.\n",
       "   - **Espaço**: Determine quanto espaço você tem disponível e planeje a disposição das plantas.\n",
       "\n",
       "3. **Preparação do Solo**:\n",
       "   - **Teste o Solo**: Um teste de solo pode ajudar a determinar o pH e quais nutrientes são necessários.\n",
       "   - **Melhorias**: Adicione composto ou esterco para enriquecer o solo.\n",
       "\n",
       "4. **Semeadura ou Transplante**:\n",
       "   - **Sementes ou Mudas**: Decida se vai começar a partir de sementes ou comprar mudas.\n",
       "   - **Profundidade e Espaçamento**: Siga as instruções de plantio específicas para cada tipo de planta.\n",
       "\n",
       "5. **Irrigação**:\n",
       "   - **Rega Regular**: Mantenha o solo úmido, mas não encharcado. A frequência da rega depende do clima e do tipo de solo.\n",
       "   - **Sistema de Irrigação**: Considere sistemas de gotejamento ou aspersão para uma rega mais eficiente.\n",
       "\n",
       "6. **Manutenção**:\n",
       "   - **Capina**: Remova ervas daninhas regularmente.\n",
       "   - **Adubação**: Aplique fertilizantes conforme necessário, baseando-se nas necessidades das plantas.\n",
       "\n",
       "7. **Controle de Pragas e Doenças**:\n",
       "   - **Monitoramento**: Fique atento a sinais de pragas e doenças.\n",
       "   - **Métodos Naturais**: Considere o uso de inseticidas naturais e práticas como rotação de culturas.\n",
       "\n",
       "8. **Colheita**:\n",
       "   - **Tempo Certo**: Colha os vegetais no momento certo para garantir o melhor sabor e qualidade.\n",
       "\n",
       "Começar uma horta requer um pouco de planejamento e esforço, mas os resultados podem ser muito gratificantes. Boa sorte e bom cultivo!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#new questions\n",
    "instructions = [{\"role\": \"user\", \"content\": \"Me dê algumas dicas de como começar uma nova horta.\"}]\n",
    "response=wrapped_chain.invoke({\"input\": format_instructions(instructions)})\n",
    "display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprimorar as perguntas\n",
    "\n",
    "Agora, faça uma pergunta sem mencionar a palavra jardim para ver se a modelo consegue entender a conversa anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Os insetos são um grupo diverso de animais invertebrados que pertencem à classe Insecta, dentro do filo Arthropoda. Eles são os animais mais numerosos do planeta, com mais de um milhão de espécies descritas, representando cerca de dois terços de todos os organismos conhecidos.\n",
       "\n",
       "### Características Principais dos Insetos:\n",
       "1. **Corpo Segmentado**: Dividido em três partes principais: cabeça, tórax e abdômen.\n",
       "2. **Três Pares de Patas**: Localizadas no tórax, permitindo mobilidade.\n",
       "3. **Um Par de Antenas**: Sensoriais, ajudando na detecção de odores e vibrações.\n",
       "4. **Exoesqueleto**: Uma estrutura externa rígida que oferece proteção e suporte.\n",
       "5. **Asas**: Muitos insetos têm asas, geralmente dois pares, permitindo o voo.\n",
       "\n",
       "### Diversidade e Habitat:\n",
       "Insetos habitam praticamente todos os ambientes da Terra, desde florestas tropicais até desertos, e de águas doces a ambientes urbanos. Essa adaptabilidade é uma das razões para sua grande diversidade.\n",
       "\n",
       "### Importância Ecológica:\n",
       "- **Polinização**: Muitos insetos, como abelhas e borboletas, são polinizadores essenciais para a reprodução de plantas.\n",
       "- **Decomposição**: Insetos como besouros e formigas ajudam a decompor matéria orgânica, reciclando nutrientes.\n",
       "- **Controle Biológico**: Alguns insetos predam outras pragas, ajudando a manter o equilíbrio ecológico.\n",
       "\n",
       "### Importância Econômica:\n",
       "- **Agricultura**: Além da polinização, insetos podem ser benéficos (por exemplo, joaninhas que comem pulgões) ou prejudiciais (por exemplo, gafanhotos que destroem colheitas).\n",
       "- **Medicina**: Alguns insetos são usados na produção de medicamentos e pesquisas.\n",
       "\n",
       "### Exemplos de Insetos:\n",
       "- **Formigas**: Sociais e altamente organizadas.\n",
       "- **Borboletas e Mariposas**: Importantes polinizadores.\n",
       "- **Moscas e Mosquitos**: Alguns podem ser vetores de doenças.\n",
       "- **Besouros**: O grupo mais diverso de insetos.\n",
       "\n",
       "Os insetos desempenham papéis cruciais em ecossistemas e na vida humana, tornando-os um grupo de estudo fascinante e vital para a compreensão da biologia e ecologia."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build on the questions\n",
    "instructions = [{\"role\": \"user\", \"content\": \"insetos\"}]\n",
    "response=wrapped_chain.invoke({\"input\": format_instructions(instructions)})\n",
    "display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encerrar a conversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Você é sempre bem-vindo! Se tiver mais perguntas ou precisar de ajuda no futuro, não hesite em retornar. Estou aqui para ajudar com qualquer informação ou dúvida que você possa ter. Tenha um ótimo dia e sucesso com a sua horta!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# finishing the conversation\n",
    "instructions = [{\"role\": \"user\", \"content\": \"É só isso, obrigado!\"}]\n",
    "response=wrapped_chain.invoke({\"input\": format_instructions(instructions)})\n",
    "display(Markdown(response)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarefa 4.3: chatbot usando modelo de prompt (LangChain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta tarefa, você usará o PromptTemplate padrão que é responsável pela elaboração dessa entrada. O LangChain oferece várias classes e funções para facilitar a elaboração e trabalho com prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  prompt for a conversational agent\n",
    "def format_prompt(actor:str, input:str):\n",
    "    formatted_prompt: List[str] = []\n",
    "    if actor == \"system\":\n",
    "        prompt_template=\"\"\"<|begin_of_text|><|start_header_id|>{actor}<|end_header_id|>\\n{input}<|eot_id|>\"\"\"\n",
    "    elif actor == \"user\":\n",
    "        prompt_template=\"\"\"<|start_header_id|>{actor}<|end_header_id|>\\n{input}<|eot_id|>\"\"\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid role: {actor}. Role must be either 'user' or 'system'.\")   \n",
    "    prompt = PromptTemplate.from_template(prompt_template)     \n",
    "    formatted_prompt.extend(prompt.format(actor=actor,input=input))\n",
    "    formatted_prompt.extend([\"<|start_header_id|>assistant<|end_header_id|>\\n\"])\n",
    "    return \"\".join(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chat user experience\n",
    "import ipywidgets as ipw\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class ChatUX:\n",
    "    \"\"\" A chat UX using IPWidgets\n",
    "    \"\"\"\n",
    "    def __init__(self, qa, retrievalChain = False):\n",
    "        self.qa = qa\n",
    "        self.name = None\n",
    "        self.b=None\n",
    "        self.retrievalChain = retrievalChain\n",
    "        self.out = ipw.Output()\n",
    "\n",
    "\n",
    "    def start_chat(self):\n",
    "        print(\"Starting chat bot\")\n",
    "        display(self.out)\n",
    "        self.chat(None)\n",
    "\n",
    "\n",
    "    def chat(self, _):\n",
    "        if self.name is None:\n",
    "            prompt = \"\"\n",
    "        else: \n",
    "            prompt = self.name.value\n",
    "        if 'q' == prompt or 'quit' == prompt or 'Q' == prompt:\n",
    "            with self.out:\n",
    "                print(\"Thank you , that was a nice chat !!\")\n",
    "            return\n",
    "        elif len(prompt) > 0:\n",
    "            with self.out:\n",
    "                thinking = ipw.Label(value=\"Thinking...\")\n",
    "                display(thinking)\n",
    "                try:\n",
    "                    if self.retrievalChain:\n",
    "                        response = self.qa.invoke({\"input\": prompt})\n",
    "                        result=response['answer']\n",
    "                    else:\n",
    "                        instructions = [{\"role\": \"user\", \"content\": prompt}]\n",
    "                        #result = self.qa.invoke({'input': format_prompt(\"user\",prompt)}) #, 'history':chat_history})\n",
    "                        result = self.qa.invoke({\"input\": format_instructions(instructions)})\n",
    "                except:\n",
    "                    result = \"No answer\"\n",
    "                thinking.value=\"\"\n",
    "                display(Markdown(result))\n",
    "                self.name.disabled = True\n",
    "                self.b.disabled = True\n",
    "                self.name = None\n",
    "\n",
    "        if self.name is None:\n",
    "            with self.out:\n",
    "                self.name = ipw.Text(description=\"You:\", placeholder='q to quit')\n",
    "                self.b = ipw.Button(description=\"Send\")\n",
    "                self.b.on_click(self.chat)\n",
    "                display(ipw.Box(children=(self.name, self.b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, inicie uma conversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chat bot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9531f337bae4f7886c0226f8263122a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start chat\n",
    "history = InMemoryChatMessageHistory() #reset chat history\n",
    "chat = ChatUX(wrapped_chain)\n",
    "chat.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: <|start_header_id|>user<|end_header_id|>\n",
      "como se emaranha duas particula atomicas <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "AI: Emaranhar duas partículas atômicas, como elétrons, átomos, ou fótons, é um conceito fundamental na mecânica quântica conhecido como \"emaranhamento quântico\". Esse fenômeno ocorre quando duas partículas se tornam interligadas de tal maneira que o estado de uma partícula não pode ser descrito independentemente do estado da outra, mesmo que as duas estejam separadas por grandes distâncias.\n",
      "\n",
      "Aqui está um resumo simplificado de como o emaranhamento pode ser alcançado:\n",
      "\n",
      "### 1. Criação de um Estado Emaranhado\n",
      "Para criar um par de partículas emaranhadas, geralmente se começa com uma interação física entre elas. Isso pode ser feito de várias maneiras, dependendo do tipo de partículas envolvidas.\n",
      "\n",
      "#### a) Emaranhamento de Fótons:\n",
      "Uma maneira comum de emaranhar fótons é através do uso de um cristal não-linear em um processo chamado conversão paramétrica descendente espontânea (SPDC). Quando um fóton de alta energia (como um fóton de laser) passa através de um cristal não-linear, ele pode ser convertido em dois fótons de menor energia. Devido à conservação do momento e da energia, esses dois fótons resultantes estarão emaranhados em suas propriedades, como polarização ou momento.\n",
      "\n",
      "#### b) Emaranhamento de Átomos ou Íons:\n",
      "Partículas atômicas como átomos ou íons podem ser emaranhadas usando interações controladas, como forças eletromagnéticas. Por exemplo, íons presos em uma armadilha de íons podem ser emaranhados usando lasers para controlar suas interações.\n",
      "\n",
      "### 2. Medição e Correlação\n",
      "Uma vez que as partículas estão emaranhadas, medir uma propriedade de uma partícula (como a polarização de um fóton ou o spin de um elétron) instantaneamente determina o estado da outra partícula, não importando a distância entre elas. Essa correlação permanece mesmo se as partículas estiverem a anos-luz de distância uma da outra.\n",
      "\n",
      "### 3. Aplicações\n",
      "O emaranhamento é uma parte crucial de muitas tecnologias quânticas emergentes, como a computação quântica, a criptografia quântica e a teleportação quântica.\n",
      "\n",
      "### Exemplo Clássico: Paradoxo EPR\n",
      "Um exemplo famoso que ilustra o emaranhamento é o paradoxo EPR (Einstein-Podolsky-Rosen). Se duas partículas estão emaranhadas de tal forma que a soma de seus spins é zero, medir o spin de uma partícula (digamos, +1/2) imediatamente determina o spin da outra partícula (-1/2), mesmo se elas estiverem em lados opostos do universo.\n",
      "\n",
      "### Conclusão\n",
      "Emaranhar partículas atômicas é um processo que envolve criar uma interação física específica entre elas, de modo que seus estados quânticos se tornem interdependentes. Esse fenômeno desafia nossas intuições clássicas sobre a separação e independência de objetos distantes e é uma das características mais intrigantes e poderosas da mecânica quântica.\n",
      "Human: <|start_header_id|>user<|end_header_id|>\n",
      "obrigado <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "AI: De nada! Se você tiver mais alguma pergunta ou precisar de mais informações sobre física quântica, emaranhamento, ou qualquer outro assunto, não hesite em perguntar. Estou aqui para ajudar!\n"
     ]
    }
   ],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tarefa 4.4: Chatbot com persona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta tarefa, o assistente de Inteligência Artificial (IA) desempenha a função de coach de carreira. Você pode usar uma mensagem do sistema para informar ao chatbot qual persona (ou função) ele vai desempenhar. Continue aproveitando a classe InMemoryChatMessageHistory para manter o contexto da conversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Existem várias áreas promissoras na Inteligência Artificial (IA) que você pode considerar, como cientista de dados, engenheiro de machine learning, especialista em visão computacional, e pesquisador de IA. Cada uma dessas áreas requer habilidades específicas e oferece oportunidades únicas para contribuir com o avanço da tecnologia. É importante avaliar suas habilidades em matemática, estatística e programação, além de considerar seu interesse em pesquisa e desenvolvimento ao escolher um caminho na IA."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \" Irá atuar como um consultor de carreira. O seu objetivo é dar conselhos de carreira aos utilizadores. Para questões que não estejam relacionadas com a carreira, não dê conselhos. Diga: Não sei\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = InMemoryChatMessageHistory() # reset history\n",
    "\n",
    "chain = prompt | chat_model | StrOutputParser()\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_history,\n",
    "    history_messages_key=\"career_chat_history\",\n",
    ")\n",
    "\n",
    "response=wrapped_chain.invoke({\"input\": \"Quais são as opções de carreira em IA?\"})\n",
    "#print(response)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Desculpe, mas não posso responder a questões que não estejam relacionadas com desenvolvimento de carreira. Sugiro procurar um mecânico de confiança ou consultar o manual do proprietário do seu veículo para dicas de manutenção básica. Se estiver procurando aprimorar habilidades profissionais relacionadas à mecânica automotiva, posso ajudar com conselhos sobre cursos e certificações na área."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response=wrapped_chain.invoke({\"input\": \"Como consertar meu carro?\"})\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Quais são as opções de carreira em IA?\n",
      "AI: Existem várias áreas promissoras na Inteligência Artificial (IA) que você pode considerar, como cientista de dados, engenheiro de machine learning, especialista em visão computacional, e pesquisador de IA. Cada uma dessas áreas requer habilidades específicas e oferece oportunidades únicas para contribuir com o avanço da tecnologia. É importante avaliar suas habilidades em matemática, estatística e programação, além de considerar seu interesse em pesquisa e desenvolvimento ao escolher um caminho na IA.\n",
      "Human: Como consertar meu carro?\n",
      "AI: Não sei responder a essa pergunta, pois não está relacionada a consultoria de carreira. Se você tiver alguma dúvida sobre como desenvolver habilidades profissionais ou escolher um caminho de carreira, ficarei feliz em ajudar. Por exemplo, se estiver interessado em mecânica automotiva como profissão, posso fornecer informações sobre como iniciar nessa área.\n",
      "Human: Como consertar meu carro?\n",
      "AI: Desculpe, mas não posso responder a questões que não estejam relacionadas com desenvolvimento de carreira. Sugiro procurar um mecânico de confiança ou consultar o manual do proprietário do seu veículo para dicas de manutenção básica. Se estiver procurando aprimorar habilidades profissionais relacionadas à mecânica automotiva, posso ajudar com conselhos sobre cursos e certificações na área.\n"
     ]
    }
   ],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, faça uma pergunta que não esteja dentro da especialidade dessa persona. O modelo não deve responder a essa pergunta e deve dar uma razão para isso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarefa 4.5 Chatbot com contexto\n",
    "\n",
    "Nesta tarefa, você pedirá que o chatbot responda perguntas com base no contexto que foi passado para ele. Você usa um arquivo CSV e o modelo do Titan Embeddings para criar um vetor representando esse contexto. Esse vetor é armazenado no Facebook AI Similarity Search (FAISS). Quando uma pergunta for feita ao chatbot, você transmitirá esse vetor de volta para o chatbot e fará com que ele recupere a resposta usando o vetor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de incorporações do Titan\n",
    "\n",
    "As incorporações representam palavras, frases ou qualquer outro item distinto como vetores em um espaço de vetores contínuo. Assim, os modelos de machine learning podem fazer operações matemáticas nessas representações e capturar as relações semânticas entre elas.\n",
    "\n",
    "Você usa incorporações para a geração aumentada de recuperação (RAG) [recurso de pesquisa de documentos](https://labelbox.com/blog/how-vector-similarity-search-works/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model configuration\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "br_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=bedrock_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS como VectorStore\n",
    "\n",
    "Para usar incorporações na pesquisa, você precisa de um armazenamento que faça pesquisas por similaridade de vetores de forma eficaz. Neste caderno, você usa o FAISS, que é um armazenamento em memória. Para armazenar vetores permanentemente, você pode usar as Bases de Conhecimento para Amazon Bedrock, pgVector, Pinecone, Weaviate ou Chroma.\n",
    "\n",
    "As APIs VectorStore do LangChain estão disponíveis [aqui](https://python.langchain.com/v0.2/docs/integrations/vectorstores/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents:loaded:size=153\n",
      "Documents:after split and chunking size=154\n",
      "vectorstore_faiss_aws:created=<langchain_community.vectorstores.faiss.FAISS object at 0x00000197978A34D0>::\n"
     ]
    }
   ],
   "source": [
    "# vector store\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "loader = CSVLoader(\"./rag_data/Amazon_SageMaker_FAQs.csv\") # --- > 219 docs with 400 chars\n",
    "documents_aws = loader.load() #\n",
    "print(f\"documents:loaded:size={len(documents_aws)}\")\n",
    "\n",
    "docs = CharacterTextSplitter(chunk_size=2000, chunk_overlap=400, separator=\",\").split_documents(documents_aws)\n",
    "\n",
    "print(f\"Documents:after split and chunking size={len(docs)}\")\n",
    "vectorstore_faiss_aws = None\n",
    "try:\n",
    "    \n",
    "    vectorstore_faiss_aws = FAISS.from_documents(\n",
    "        documents=docs,\n",
    "        embedding = br_embeddings, \n",
    "        #**k_args\n",
    "    )\n",
    "\n",
    "    print(f\"vectorstore_faiss_aws:created={vectorstore_faiss_aws}::\")\n",
    "\n",
    "except ValueError as error:\n",
    "    if  \"AccessDeniedException\" in str(error):\n",
    "        print(f\"\\x1b[41m{error}\\\n",
    "        \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "         \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "         \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")      \n",
    "        class StopExecution(ValueError):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "        raise StopExecution        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazer um teste rápido com pouco código \n",
    "\n",
    "Você pode usar uma classe wrapper fornecida pelo LangChain para consultar o armazenamento do banco de dados de vetores e retornar os documentos relevantes. Isso processa uma cadeia de QA com todos os valores padrão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sim, o Amazon SageMaker suporta a linguagem de programação R. Ele oferece kernels de Jupyter Notebook para R, permitindo que os usuários escrevam e executem scripts em R para tarefas de machine learning. Você pode usar o R no SageMaker para preparar dados, treinar modelos, e fazer deploy, tudo dentro de um ambiente totalmente gerenciado. Além disso, o SageMaker Studio, que é a interface web integrada para desenvolvimento de ML, também suporta notebooks em R, facilitando ainda mais o trabalho com essa linguagem na plataforma."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_llm=ChatBedrock(\n",
    "    model_id=\"us.amazon.nova-premier-v1:0\" , \n",
    "    client=bedrock_client)\n",
    "# wrapper store faiss\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss_aws)\n",
    "display(Markdown(wrapper_store_faiss.query(\"O SageMaker suporta o R\", llm=chat_llm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicação de chatbot\n",
    "\n",
    "Para o chatbot, você precisa de gerenciamento de contexto, histórico, armazenamentos de vetores e muitos outros componentes. Comece criando uma cadeia de Geração aumentada de recuperação (RAG) com suporte a contexto.\n",
    "\n",
    "Ela usa as funções **create_stuff_documents_chain** e **create_retrieval_chain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parâmetros e funções usados para a RAG\n",
    "\n",
    "- **Recuperador:** você usou `VectorStoreRetriever`, que é apoiado por um `VectorStore`. Para recuperar texto, há dois tipos de pesquisa para escolher: `\"similarity\"` ou `\"mmr\"`. `search_type=\"similarity\"` usa a pesquisa por similaridade no objeto recuperador, onde seleciona vetores de fragmentos de texto que são mais semelhantes ao vetor da pergunta.\n",
    "\n",
    "- **create_stuff_documents_chain** especifica como o contexto recuperado é alimentado em um prompt e no LLM. Os documentos recuperados são \"preenchidos\" como contexto sem qualquer resumo ou outro processamento no prompt.\n",
    "\n",
    "- **create_retrieval_chain** adiciona a etapa de recuperação, propaga o contexto recuperado pela cadeia e o fornece junto com a resposta final. \n",
    "\n",
    "Se a pergunta feita estiver fora do escopo do contexto, o modelo responderá que não sabe a resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'O que é o Sagemaker?', 'context': [Document(id='5cd86b93-bf5d-43aa-8986-ff1b203230d1', metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 12}, page_content='ï»¿What is Amazon SageMaker?: What is Amazon SageMaker Studio?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Amazon SageMaker Studio provides a single, web-based visual interface where you can perform all ML development steps. SageMaker Studio gives you complete access, control, and visibility into each step required to prepare data and build, train, and deploy models. You can quickly upload data, create new notebooks, train and tune models, move back and forth between steps to adjust experiments, compare results, and deploy models to production all in one place, making you much more productive. All ML development activities including notebooks, experiment management, automatic model creation, debugging and profiling, and model drift detection can be performed within the unified SageMaker Studio visual interface.'), Document(id='98f6577d-e445-4922-b96a-ac3b01c83e6e', metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 141}, page_content='ï»¿What is Amazon SageMaker?: What is Amazon SageMaker Neo?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Amazon SageMaker Neo enables ML models to train once and run anywhere in the cloud and at the edge. SageMaker Neo automatically optimizes models built with popular deep learning frameworks that can be used to deploy on multiple hardware platforms. Optimized models run up to 25 times faster and consume less than a tenth of the resources of typical ML models.'), Document(id='358539d6-b33a-47a6-b8fc-6ca9ac8ba2a4', metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 92}, page_content='ï»¿What is Amazon SageMaker?: What algorithms does Amazon SageMaker use to generate models?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Amazon SageMaker includes built-in algorithms for linear regression, logistic regression, k-means clustering, principal component analysis, factorization machines, neural topic modeling, latent dirichlet allocation, gradient boosted trees, sequence2sequence, time-series forecasting, word2vec, and image classification. SageMaker also provides optimized Apache MXNet, Tensorflow, Chainer, PyTorch, Gluon, Keras, Horovod, Scikit-learn, and Deep Graph Library containers. In addition, Amazon SageMaker supports your custom training algorithms provided through a Docker image adhering to the documented specification.'), Document(id='b1072a1d-7b0c-426c-86c3-dddfb526dacb', metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 7}, page_content='ï»¿What is Amazon SageMaker?: What if I have my own notebook, training, or hosting environment?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Amazon SageMaker provides a full end-to-end workflow, but you can continue to use your existing tools with SageMaker. You can easily transfer the results of each stage in and out of SageMaker as your business requirements dictate.')], 'answer': 'Amazon SageMaker é um serviço gerenciado para criar, treinar e implantar modelos de machine learning. Ele oferece infraestrutura, ferramentas e fluxos de trabalho integrados.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"Você é um assistente para as tarefas de resposta a perguntas.\"\n",
    "    \"Utilize os seguintes excertos de contexto recuperados para responder à questão.\"\n",
    "    \"Se não souber a resposta, diga que não sabe.\"\n",
    "    \"Utilize no máximo três frases e mantenha a resposta concisa.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retriever=vectorstore_faiss_aws.as_retriever()\n",
    "question_answer_chain = create_stuff_documents_chain(chat_llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"O que é o Sagemaker?\"})\n",
    "print(response) # shows the document chunks consulted to come up with the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, inicie uma conversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chat bot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc37dc1929b34176806d861e5f2b5aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = ChatUX(rag_chain, retrievalChain=True)\n",
    "chat.start_chat()  # Only answers will be shown here, and not the citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Você usou o LLM Titan para criar uma interface conversacional com os seguintes padrões:\n",
    "\n",
    "- Chatbot (básico: sem contexto)\n",
    "- Chatbot usando modelo de prompt (LangChain)\n",
    "- Chatbot com personas\n",
    "- Chatbot com contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimente você mesmo\n",
    "\n",
    "- Altere os prompts para seu caso de uso específico e avalie o resultado de diferentes modelos.\n",
    "- Teste o comprimento do token para entender a latência e a responsividade do serviço.\n",
    "- Aplique diferentes princípios de engenharia de prompts para gerar resultados melhores.\n",
    "\n",
    "### Limpeza\n",
    "\n",
    "Você concluiu este caderno. Passe para a próxima parte do laboratório da seguinte forma:\n",
    "\n",
    "- Feche este arquivo de caderno e continue com a **Tarefa 5**."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
